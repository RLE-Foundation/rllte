<div align=center>
<br>
<img src='./assets/images/logo_horizontal.svg' style="width: 75%">
<br>
RLLTE: å¼ºåŒ–å­¦ä¹ é•¿æœŸæ¼”è¿›è®¡åˆ’

<h3> <a href=""> è®ºæ–‡ </a> |
<a href="https://docs.rllte.dev/api/"> æ–‡æ¡£ </a> |
<a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> ç¤ºä¾‹ </a> |
<a href="https://github.com/RLE-Foundation/rllte/discussions"> è®ºå› </a> |
<a href="https://hub.rllte.dev/"> åŸºçº¿ </a></h3>

<img src="https://img.shields.io/badge/License-MIT-%230677b8"> <img src="https://img.shields.io/badge/GPU-NVIDIA-%2377b900"> <img src="https://img.shields.io/badge/NPU-Ascend-%23c31d20"> <img src="https://img.shields.io/badge/Python-%3E%3D3.8-%2335709F"> <img src="https://img.shields.io/badge/Docs-Passing-%23009485"> <img src="https://img.shields.io/badge/Codestyle-Black-black"> <img src="https://img.shields.io/badge/PyPI-0.0.1-%23006DAD"> <img src="https://img.shields.io/badge/Coverage-98.00%25-green"> 

| [English](README.md) | [ä¸­æ–‡](docs/README-zh-Hans.md) |

</div>

# Contents
- [æ¦‚è¿°](#overview)
- [å¿«é€Ÿå…¥é—¨](#quick-start)
  + [å®‰è£…](#installation)
  + [å¿«é€Ÿè®­ç»ƒå†…ç½®ç®—æ³•](#fast-training-with-built-in-algorithms)
    - [è¿ç”¨NVIDIA GPU](#on-nvidia-gpu)
    - [è¿ç”¨HUAWEI NPU](#on-huawei-npu)
  + [ä¸‰æ­¥åˆ›å»ºæ‚¨çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“](#three-steps-to-create-your-rl-agent)
  + [ç®—æ³•è§£è€¦ä¸æ¨¡å—æ›¿ä»£](#algorithm-decoupling-and-module-replacement)
- [åŠŸèƒ½åˆ—è¡¨ (éƒ¨åˆ†)](#function-list-part)
  + [å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“](#rl-agents)
  + [å†…åœ¨å¥–åŠ±æ¨¡å—](#intrinsic-reward-modules)
- [RLLTE ç”Ÿæ€ç¯å¢ƒ](#rllte-ecosystem)
- [API æ–‡æ¡£](#api-documentation)
- [å¼•ç”¨é¡¹ç›®](#cite-the-project)
- [å¦‚ä½•è´¡çŒ®](#how-to-contribute)
- [è‡´è°¢](#acknowledgment)

# æ¦‚è¿°
å—é€šä¿¡é¢†åŸŸé•¿æœŸæ¼”è¿›ï¼ˆLTEï¼‰æ ‡å‡†é¡¹ç›®çš„å¯å‘ï¼ŒRLLTEæ—¨åœ¨æä¾›ç”¨äºæ¨è¿›RLç ”ç©¶å’Œåº”ç”¨çš„å¼€å‘ç»„ä»¶å’Œæ¶æ„ã€‚é™¤äº†æä¾›ä¸€æµçš„ç®—æ³•å®ç°å¤–ï¼Œ**RLLTE**è¿˜èƒ½å¤Ÿå……å½“å¼€å‘ç®—æ³•çš„å·¥å…·åŒ…ã€‚

<div align="center">
<a href="https://youtu.be/ShVdiHHyXFM" rel="nofollow">
<img src='./assets/images/youtube.png' style="width: 70%">
</a>
<br>
RLLTEç®€ä»‹.
</div>

ä¸ºä»€ä¹ˆä½¿ç”¨ **RLLTE**?
- ğŸ§¬ æä¾›æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸æŠ€å·§;
- ğŸï¸ æä¾›å®Œæ•´çš„ç”Ÿæ€ç¯å¢ƒï¼Œæ¡†æ¶æ”¯æŒè®¾è®¡ä»»åŠ¡ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œè¯„ä¼°æ¨¡å‹ä»¥åŠéƒ¨ç½²ç®—æ³• (TensorRT, CANN, ...);
- ğŸ§± é’ˆå¯¹å®Œæ•´è§£è€¦ç®—æ³•ï¼Œæå‡ºæ¨¡å—åŒ–è®¾è®¡;
- ğŸš€ æå‡ºç®—æ³•ä¼˜åŒ–ï¼Œæ”¯æŒç¡¬ä»¶åŠ é€Ÿ;
- âš™ï¸ æ”¯æŒè‡ªå®šä¹‰ä»»åŠ¡å’Œæ¨¡å—;
- ğŸ–¥ï¸ æ”¯æŒåŒ…æ‹¬GPUå’ŒNPUçš„å¤šç§ç®—åŠ›è®¾å¤‡;
- ğŸ’¾ å¤§é‡å¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯• ([rllte-hub](https://hub.rllte.dev));
- ğŸ‘¨â€âœˆï¸ åŸºäºå¤§è¯­è¨€æ¨¡å‹æ‰“é€ çš„copilot.

é¡¹ç›®ç»“æ„å¦‚ä¸‹:
<div align=center>
<img src='./assets/images/structure.svg' style="width: 100%">
</div>

æœ‰å…³è¿™äº›æ¨¡å—çš„æ›´è¯¦ç»†æè¿°ï¼Œè¯·å‚é˜…[APIæ–‡æ¡£](https://docs.rllte.dev/api).

# å¿«é€Ÿå…¥é—¨
## å®‰è£…
- å‰ç½®æ¡ä»¶

å½“å‰ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`Python>=3.8`ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š
``` sh
conda create -n rllte python=3.8
```

- é€šè¿‡ `pip`

æ‰“å¼€ç»ˆç«¯é€šè¿‡`pip`å®‰è£… **rllte**:
``` shell
pip install rllte-core # å®‰è£…åŸºæœ¬æ¨¡å—
pip install rllte-core[envs] # å®‰è£…å¼ºåŒ–å­¦ä¹ é¢„è®¾ç¯å¢ƒ
```

- é€šè¿‡ `git`

å¼€å¯ç»ˆç«¯ä»[GitHub]ä¸­å¤åˆ¶ä»“åº“(https://github.com/RLE-Foundation/rllte):
``` sh
git clone https://github.com/RLE-Foundation/rllte.git
```
åœ¨è¿™ä¹‹å, è¿è¡Œä»¥ä¸‹å‘½ä»¤è¡Œå®‰è£…æ‰€éœ€çš„åŒ…:
``` sh
pip install -e . # å®‰è£…åŸºæœ¬æ¨¡å—
pip install -e .[envs] # å®‰è£…å¼ºåŒ–å­¦ä¹ é¢„è®¾ç¯å¢ƒ
```

æ›´è¯¦ç»†çš„å®‰è£…è¯´æ˜, è¯·å‚é˜…, [å…¥é—¨æŒ‡å—](https://docs.rllte.dev/getting_started).

## å¿«é€Ÿè®­ç»ƒå†…ç½®ç®—æ³•
**RLLTE** æä¾›äº†å¸¸ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®ç°ï¼Œå¹¶ä¸”é™„å¸¦ç®€å•çš„ç•Œé¢ã€‚
### è¿ç”¨NVIDIA GPU
å‡å¦‚éœ€è¦é€šè¿‡ [DrQ-v2](https://openreview.net/forum?id=_SJ-_yyes8) æ¥å­¦ä¹  [DeepMind Control Suite](https://github.com/deepmind/dm_control)ä»»åŠ¡, åªéœ€ç¼–å†™å¦‚ä¸‹ `train.py`æ–‡ä»¶:

``` python
# import `env` and `agent` module
from rllte.env import make_dmc_env 
from rllte.agent import DrQv2

if __name__ == "__main__":
    device = "cuda:0"
    # åˆ›å»º env, `eval_env` å¯é€‰
    env = make_dmc_env(env_id="cartpole_balance", device=device)
    eval_env = make_dmc_env(env_id="cartpole_balance", device=device)
    # åˆ›å»º agent
    agent = DrQv2(env=env, eval_env=eval_env, device=device, tag="drqv2_dmc_pixel")
    # å¼€å§‹è®­ç»ƒ
    agent.train(num_train_steps=500000, log_interval=1000)
```
è¿è¡Œ`train.py`æ–‡ä»¶ï¼Œå°†ä¼šå¾—åˆ°å¦‚ä¸‹ç»“æœ:

<div align=center>
<img src='./assets/images/rl_training_gpu.gif' style="filter: drop-shadow(0px 0px 7px #000);">
</div>

### è¿ç”¨HUAWEI NPU
ä¸ä¸Šè¿°æ¡ˆä¾‹ç±»ä¼¼, å¦‚æœéœ€è¦åœ¨ HUAWEI NPU ä¸Šè®­ç»ƒæ™ºèƒ½ä½“, åªéœ€å°† `cuda` æ›¿æ¢ä¸º `npu`:
``` python
device = "cuda:0" -> device = "npu:0"
```

## ä¸‰æ­¥åˆ›å»ºæ‚¨çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
å¼€å‘è€…å€ŸåŠ©**RLLTE**ï¼Œåªéœ€ä¸‰æ­¥å°±å¯ä»¥å®ç°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚æ¥ä¸‹æ¥è¿™ä¸ªä¾‹å­å°†å±•ç¤ºå¦‚ä½•é’ˆå¯¹ Atari games å®ç° dvantage Actor-Critic (A2C) ç®—æ³•ã€‚ 
- é¦–å…ˆ, å£°æ˜ä¸€ä¸ªåŸå‹:
``` py
from rllte.common.prototype import OnPolicyAgent
```
- å…¶æ¬¡, å¯¼å…¥å¿…è¦çš„æ¨¡å—:
``` py
from rllte.xploit.encoder import MnihCnnEncoder
from rllte.xploit.policy import OnPolicySharedActorCritic
from rllte.xploit.storage import VanillaRolloutStorage
from rllte.xplore.distribution import Categorical
```
- æœ€åè¿è¡Œè¯¥ç­–ç•¥çš„ `.describe` å‡½æ•°, è¿è¡Œç»“æœå¦‚ä¸‹:
``` py
OnPolicySharedActorCritic.describe()
# Output:
# ================================================================================
# Name       : OnPolicySharedActorCritic
# Structure  : self.encoder (shared by actor and critic), self.actor, self.critic
# Forward    : obs -> self.encoder -> self.actor -> actions
#            : obs -> self.encoder -> self.critic -> values
#            : actions -> log_probs
# Optimizers : self.optimizers['opt'] -> (self.encoder, self.actor, self.critic)
# ================================================================================
```
è¿™å°†ä¼šå±•ç¤ºç­–ç•¥çš„ç»“æ„ã€‚æœ€åï¼Œå°†ä¸Šè¿°æ¨¡å—æ•´åˆåˆ°ä¸€èµ·å¹¶ä¸”ç¼–è¾‘ `.update` å‡½æ•°:
``` py
from torch import nn
import torch as th

class A2C(OnPolicyAgent):
    def __init__(self, env, tag, seed, device, num_steps) -> None:
        super().__init__(env=env, tag=tag, seed=seed, device=device, num_steps=num_steps)
        # åˆ›å»ºæ¨¡å—
        encoder = MnihCnnEncoder(observation_space=env.observation_space, feature_dim=512)
        policy = OnPolicySharedActorCritic(observation_space=env.observation_space,
                                           action_space=env.action_space,
                                           feature_dim=512,
                                           opt_class=th.optim.Adam,
                                           opt_kwargs=dict(lr=2.5e-4, eps=1e-5),
                                           init_fn="xavier_uniform"
                                           )
        storage = VanillaRolloutStorage(observation_space=env.observation_space,
                                        action_space=env.action_space,
                                        device=device,
                                        storage_size=self.num_steps,
                                        num_envs=self.num_envs,
                                        batch_size=256
                                        )
        # è®¾ç½®æ‰€æœ‰æ¨¡å—
        self.set(encoder=encoder, policy=policy, storage=storage, distribution=Categorical)
    
    def update(self):
        for _ in range(4):
            for batch in self.storage.sample():
                # è¯„ä¼°é‡‡æ ·çš„åŠ¨ä½œ
                new_values, new_log_probs, entropy = self.policy.evaluate_actions(obs=batch.observations, actions=batch.actions)
                # ç­–ç•¥æŸå¤±
                policy_loss = - (batch.adv_targ * new_log_probs).mean()
                # ä»·å€¼æŸå¤±
                value_loss = 0.5 * (new_values.flatten() - batch.returns).pow(2).mean()
                # æ›´æ–°
                self.policy.optimizers['opt'].zero_grad(set_to_none=True)
                (value_loss * 0.5 + policy_loss - entropy * 0.01).backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
                self.policy.optimizers['opt'].step()
```
ç„¶å, é€šè¿‡ä»¥ä¸‹æ–¹æ³•æ˜¯è®­ç»ƒè¯¥æ™ºèƒ½ä½“
``` py
from rllte.env import make_atari_env
if __name__ == "__main__":
    device = "cuda"
    env = make_atari_env("PongNoFrameskip-v4", num_envs=8, seed=0, device=device)
    agent = A2C(env=env, tag="a2c_atari", seed=0, device=device, num_steps=128)
    agent.train(num_train_steps=10000000)
```
é€šè¿‡å¦‚ä¸Šä¾‹å­å¯ä»¥å‘ç°,  åˆ©ç”¨ **RLLTE** åªéœ€å°‘æ•°å‡ è¡Œä»£ç ä¾¿å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“. 

## ç®—æ³•è§£è€¦ä¸æ¨¡å—æ›¿ä»£
**RLLTE** è®¸å¯å¼€å‘è€…å°†é¢„è®¾å¥½çš„æ¨¡å—æ›¿æ¢, ä»¥ä¾¿äºè¿›è¡Œç®—æ³•æ€§èƒ½æ¯”è¾ƒå’Œä¼˜åŒ–ã€‚å¼€å‘è€…å¯ä»¥å°†é¢„è®¾æ¨¡å—æ›¿æ¢æˆåˆ«çš„ç±»å‹çš„å†…ç½®æ¨¡å—æˆ–è€…è‡ªå®šä¹‰æ¨¡å—ã€‚å‡è®¾æˆ‘ä»¬æƒ³è¦å¯¹æ¯”ä¸åŒç¼–ç å™¨çš„æ•ˆæœ. ç¥éœ€è¦è°ƒç”¨å…¶ä¸­ `.set` å‡½æ•°:
``` py
from rllte.xploit.encoder import EspeholtResidualEncoder
encoder = EspeholtResidualEncoder(...)
agent.set(encoder=encoder)
```
**RLLTE** æ¡†æ¶ç®€ä¾¿, ç»™äºˆå¼€å‘è€…ä»¬æœ€å¤§ç¨‹åº¦çš„è‡ªç”±ã€‚æ›´å¤šè¯¦ç»†è¯´æ˜, è¯·å‚è€ƒ [æ•™ç¨‹](https://docs.rllte.dev/tutorials)ã€‚

# åŠŸèƒ½åˆ—è¡¨ (éƒ¨åˆ†)
## å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
|     ç±»å‹    |  ç®—æ³• | è¿ç»­çŠ¶æ€ | ç¦»æ•£çŠ¶æ€ | å¤šé‡äºŒè¿›åˆ¶ | å¤šé‡ç¦»æ•£ | å¤šçº¿ç¨‹ | NPU |ğŸ’°|ğŸ”­|
|:-----------:|:------:|:---:|:----:|:----:|:----:|:------:|:---:|:------:|:---:|
| On-Policy   | [A2C](https://arxiv.org/abs/1602.01783)    | âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| On-Policy   | [PPO](https://arxiv.org/pdf/1707.06347)    | âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| On-Policy   | [DrAC](https://proceedings.neurips.cc/paper/2021/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âœ”ï¸   |
| On-Policy   | [DAAC](http://proceedings.mlr.press/v139/raileanu21a/raileanu21a.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âŒ   |
| On-Policy   | [DrDAAC](https://proceedings.neurips.cc/paper/2021/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âœ”ï¸   |
| On-Policy   | [PPG](http://proceedings.mlr.press/v139/cobbe21a/cobbe21a.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    |  âŒ   | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âŒ   |
| Off-Policy  | [DQN](https://training.incf.org/sites/default/files/2023-05/Human-level%20control%20through%20deep%20reinforcement%20learning.pdf) | âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âŒ   |
| Off-Policy  | [DDPG](https://arxiv.org/pdf/1509.02971.pdf?source=post_page---------------------------)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| Off-Policy  | [SAC](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| Off-Policy  | [TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| Off-Policy  | [DrQ-v2](https://arxiv.org/pdf/2107.09645.pdf?utm_source=morioh.com)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âŒ    | âœ”ï¸   |âœ”ï¸    |âœ”ï¸    |
| Distributed | [IMPALA](http://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf) | âœ”ï¸   | âœ”ï¸    | âŒ    | âŒ    | âœ”ï¸    | âŒ   |âŒ    |âŒ    |

> - ğŸŒ: å¼€å‘ä¸­;
> - ğŸ’°: æ”¯æŒ intrinsic reward shaping;
> - ğŸ”­: æ”¯æŒ observation augmentationã€‚


## å†…åœ¨å¥–åŠ±æ¨¡å—
| **ç±»å‹** 	| **æ¨¡å—** 	|
|---	|---	|
| Count-based 	| [PseudoCounts](https://arxiv.org/pdf/2002.06038), [RND](https://arxiv.org/pdf/1810.12894.pdf) 	|
| Curiosity-driven 	| [ICM](http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf), [GIRM](http://proceedings.mlr.press/v119/yu20d/yu20d.pdf), [RIDE](https://arxiv.org/pdf/2002.12292) 	|
| Memory-based 	| [NGU](https://arxiv.org/pdf/2002.06038) 	|
| Information theory-based 	| [RE3](http://proceedings.mlr.press/v139/seo21a/seo21a.pdf), [RISE](https://ieeexplore.ieee.org/abstract/document/9802917/), [REVD](https://openreview.net/pdf?id=V2pw1VYMrDo) 	|

è¯¦ç»†æ¡ˆä¾‹è¯·å‚è€ƒ [Tutorials: Use Intrinsic Reward and Observation Augmentation](https://docs.rllte.dev/tutorials/data_augmentation).

# RLLTE ç”Ÿæ€ç¯å¢ƒ
æ¬¢è¿ä½¿ç”¨RLLTE ç”Ÿæ€ç¯å¢ƒ, ä¸ºæ‚¨æ‰“é€ èˆ’é€‚ä¾¿åˆ©çš„å¹³å°:

- [Hub](https://docs.rllte.dev/benchmarks/): æä¾›å¿«é€Ÿè®­ç»ƒçš„ API æ¥å£ä»¥åŠå¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•.
- [Evaluation](https://docs.rllte.dev/api/tutorials/): æä¾›å¯ä¿¡èµ–çš„æ¨¡å‹è¯„ä¼°æ ‡å‡†ã€‚
- [Env](https://docs.rllte.dev/api/tutorials/): æä¾›å°è£…å®Œå–„çš„ç¯å¢ƒã€‚
- [Deployment](https://docs.rllte.dev/api/tutorials/): æä¾›ä¾¿æ·çš„ç®—æ³•éƒ¨ç½²æ¥å£ã€‚
- [Pre-training](https://docs.rllte.dev/api/tutorials/): æä¾›å¤šç§å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒçš„æ–¹å¼ã€‚
- [Copilot](https://docs.rllte.dev/copilot): æä¾›å¤§è¯­è¨€æ¨¡å‹ copilotã€‚

# API æ–‡æ¡£
è¯·å‚é˜…æˆ‘ä»¬è¯¦ç»†çš„ API æ–‡æ¡£: [https://docs.rllte.dev/](https://docs.rllte.dev/)
<div align=center>
<img src='./assets/images/docs.gif' style="width: 100%">
</div>

# å¦‚ä½•è´¡çŒ®
æ¬¢è¿å‚ä¸è´¡çŒ®æˆ‘ä»¬çš„é¡¹ç›®! åœ¨æ‚¨å‡†å¤‡ä¹‹å‰, è¯·å…ˆå‚é˜…[CONTRIBUTING.md](https://github.com/RLE-Foundation/rllte/blob/main/CONTRIBUTING.md)ã€‚

# å¼•ç”¨é¡¹ç›®
å¦‚æœæ‚¨æƒ³åœ¨ç ”ç©¶ä¸­å¼•ç”¨ **RLLTE**, è¯·å‚è€ƒå¦‚ä¸‹æ ¼å¼:
``` tex
@software{rllte,
  author = {Mingqi Yuan, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, and Wenjun Zeng},
  title = {RLLTE: Long-Term Evolution Project of Reinforcement Learning},
  url = {https://github.com/RLE-Foundation/rllte},
  year = {2023},
}
```

# è‡´è°¢
è¯¥é¡¹ç›®ç”± [é¦™æ¸¯ç†å·¥å¤§å­¦](http://www.polyu.edu.hk/), [ä¸œæ–¹ç†å·¥é«˜ç­‰ç ”ç©¶é™¢](http://www.eias.ac.cn/), ä»¥åŠ [FLW-Foundation](FLW-Foundation)èµåŠ©ã€‚ [ä¸œæ–¹ç†å·¥é«˜ç­‰ç ”ç©¶é™¢](https://hpc.eias.ac.cn/) æä¾› GPU è®¡ç®—å¹³å°, [åä¸ºå¼‚è…¾](https://www.hiascend.com/) æä¾› NPU è®¡ç®—å¹³å°ã€‚ è¯¥é¡¹ç›®çš„éƒ¨åˆ†ä»£ç å‚è€ƒäº†å…¶ä»–ä¼˜ç§€çš„å¼€æºé¡¹ç›®. è¯·å‚è§ [ACKNOWLEDGMENT.md](https://github.com/RLE-Foundation/rllte/blob/main/ACKNOWLEDGMENT.md).

<div align=center>
<br>
<img src='./assets/images/logo_horizontal.svg' style="width: 75%">
<br>
RLLTE: å¼ºåŒ–å­¦ä¹ é•¿æœŸæ¼”è¿›è®¡åˆ’

<h3> <a href=""> è®ºæ–‡ </a> |
<a href="https://docs.rllte.dev/api/"> æ–‡æ¡£ </a> |
<a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> ç¤ºä¾‹ </a> |
<a href="https://github.com/RLE-Foundation/rllte/discussions"> è®ºå› </a> |
<a href="https://hub.rllte.dev/"> åŸºçº¿ </a></h3>

<img src="https://img.shields.io/badge/License-MIT-%230677b8"> <img src="https://img.shields.io/badge/GPU-NVIDIA-%2377b900"> <img src="https://img.shields.io/badge/NPU-Ascend-%23c31d20"> <img src="https://img.shields.io/badge/Python-%3E%3D3.8-%2335709F"> <img src="https://img.shields.io/badge/Docs-Passing-%23009485"> <img src="https://img.shields.io/badge/Codestyle-Black-black"> <img src="https://img.shields.io/badge/PyPI-0.0.1-%23006DAD"> <img src="https://img.shields.io/badge/Coverage-98.00%25-green"> 

| [English](README.md) | [ä¸­æ–‡](docs/README-zh-Hans.md) |

</div>

# Contents
- [æ¦‚è¿°](#overview)
- [å¿«é€Ÿå…¥é—¨](#quick-start)
  + [å®‰è£…](#installation)
  + [å¿«é€Ÿè®­ç»ƒ](#fast-training-with-built-in-algorithms)
    - [è¿ç”¨NVIDIA GPU](#on-nvidia-gpu)
    - [è¿ç”¨HUAWEI NPU](#on-huawei-npu)
  + [ä¸‰æ­¥åˆ›å»ºæ‚¨çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“](#three-steps-to-create-your-rl-agent)
  + [ç®—æ³•è§£è€¦ä¸æ¨¡å—æ›¿ä»£](#algorithm-decoupling-and-module-replacement)
- [åŠŸèƒ½åˆ—è¡¨ (éƒ¨åˆ†)](#function-list-part)
  + [å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“](#rl-agents)
  + [å†…åœ¨å¥–åŠ±æ¨¡å—](#intrinsic-reward-modules)
- [RLLTEç”Ÿæ€ç¯å¢ƒ](#rllte-ecosystem)
- [API æ–‡æ¡£](#api-documentation)
- [å¼•ç”¨é¡¹ç›®](#cite-the-project)
- [å¦‚ä½•è´¡çŒ®](#how-to-contribute)
- [è‡´è°¢](#acknowledgment)

# æ¦‚è¿°
å—é€šä¿¡é¢†åŸŸé•¿æœŸæ¼”è¿›ï¼ˆLTEï¼‰æ ‡å‡†é¡¹ç›®çš„å¯å‘ï¼ŒRLLTEæ—¨åœ¨æä¾›ç”¨äºæ¨è¿›RLç ”ç©¶å’Œåº”ç”¨çš„å¼€å‘ç»„ä»¶å’Œå·¥ç¨‹æ ‡å‡†ã€‚é™¤äº†æä¾›ä¸€æµçš„ç®—æ³•å®ç°å¤–ï¼Œ**RLLTE**è¿˜èƒ½å¤Ÿå……å½“å¼€å‘ç®—æ³•çš„å·¥å…·åŒ…ã€‚

<div align="center">
<a href="https://youtu.be/ShVdiHHyXFM" rel="nofollow">
<img src='./assets/images/youtube.png' style="width: 70%">
</a>
<br>
RLLTEç®€ä»‹.
</div>

**RLLTE**é¡¹ç›®ç‰¹è‰²ï¼š
- ğŸ§¬ é•¿æœŸæ¼”è¿›ä»¥æä¾›æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸æŠ€å·§ï¼›
- ğŸï¸ ä¸°å¯Œå®Œå¤‡çš„é¡¹ç›®ç”Ÿæ€ï¼Œæ”¯æŒä»»åŠ¡è®¾è®¡ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹è¯„ä¼°ä»¥åŠæ¨¡å‹éƒ¨ç½² (TensorRT, CANN, ...)ï¼›
- ğŸ§± é«˜åº¦æ¨¡å—åŒ–çš„è®¾è®¡ä»¥å®ç°RLç®—æ³•çš„å®Œå…¨è§£è€¦ï¼›
- ğŸš€ ä¼˜åŒ–çš„å·¥ä½œæµç”¨äºç¡¬ä»¶åŠ é€Ÿï¼›
- âš™ï¸ æ”¯æŒè‡ªå®šä¹‰ç¯å¢ƒå’Œæ¨¡å—ï¼›
- ğŸ–¥ï¸ æ”¯æŒåŒ…æ‹¬GPUå’ŒNPUçš„å¤šç§ç®—åŠ›è®¾å¤‡ï¼›
- ğŸ’¾ å¤§é‡å¯é‡ç”¨çš„åŸºçº¿æ•°æ® ([rllte-hub](https://hub.rllte.dev))ï¼›
- ğŸ‘¨â€âœˆï¸ åŸºäºå¤§è¯­è¨€æ¨¡å‹æ‰“é€ çš„Copilotã€‚

é¡¹ç›®ç»“æ„å¦‚ä¸‹:
<div align=center>
<img src='./assets/images/structure.svg' style="width: 100%">
</div>

æœ‰å…³è¿™äº›æ¨¡å—çš„è¯¦ç»†æè¿°ï¼Œè¯·å‚é˜…[APIæ–‡æ¡£](https://docs.rllte.dev/api)ã€‚

# å¿«é€Ÿå…¥é—¨
## å®‰è£…
- å‰ç½®æ¡ä»¶

å½“å‰ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`Python>=3.8`ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š
``` sh
conda create -n rllte python=3.8
```

- é€šè¿‡ `pip`

æ‰“å¼€ç»ˆç«¯é€šè¿‡`pip`å®‰è£… **rllte**ï¼š
``` shell
pip install rllte-core # å®‰è£…åŸºæœ¬æ¨¡å—
pip install rllte-core[envs] # å®‰è£…é¢„è®¾çš„ä»»åŠ¡ç¯å¢ƒ
```

- é€šè¿‡ `git`

å¼€å¯ç»ˆç«¯ä»[GitHub]ä¸­å¤åˆ¶ä»“åº“(https://github.com/RLE-Foundation/rllte)ï¼š
``` sh
git clone https://github.com/RLE-Foundation/rllte.git
```
åœ¨è¿™ä¹‹å, è¿è¡Œä»¥ä¸‹å‘½ä»¤è¡Œå®‰è£…æ‰€éœ€çš„åŒ…ï¼š
``` sh
pip install -e . # å®‰è£…åŸºæœ¬æ¨¡å—
pip install -e .[envs] # å®‰è£…é¢„è®¾çš„ä»»åŠ¡ç¯å¢ƒ
```

æ›´è¯¦ç»†çš„å®‰è£…è¯´æ˜, è¯·å‚é˜…, [å…¥é—¨æŒ‡å—](https://docs.rllte.dev/getting_started).

## å¿«é€Ÿè®­ç»ƒå†…ç½®ç®—æ³•
**RLLTE**ä¸ºå¹¿å—è®¤å¯çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•æä¾›äº†é«˜è´¨é‡çš„å®ç°ï¼Œå¹¶ä¸”è®¾è®¡äº†ç®€å•å‹å¥½çš„ç•Œé¢ç”¨äºåº”ç”¨æ„å»ºã€‚
### ä½¿ç”¨NVIDIA GPU
å‡å¦‚æˆ‘ä»¬è¦ç”¨ [DrQ-v2](https://openreview.net/forum?id=_SJ-_yyes8)ç®—æ³•è§£å†³ [DeepMind Control Suite](https://github.com/deepmind/dm_control)ä»»åŠ¡, åªéœ€ç¼–å†™å¦‚ä¸‹ `train.py`æ–‡ä»¶ï¼š

``` python
# import `env` and `agent` module
from rllte.env import make_dmc_env 
from rllte.agent import DrQv2

if __name__ == "__main__":
    device = "cuda:0"
    # åˆ›å»º env, `eval_env` å¯é€‰
    env = make_dmc_env(env_id="cartpole_balance", device=device)
    eval_env = make_dmc_env(env_id="cartpole_balance", device=device)
    # åˆ›å»º agent
    agent = DrQv2(env=env, eval_env=eval_env, device=device, tag="drqv2_dmc_pixel")
    # å¼€å§‹è®­ç»ƒ
    agent.train(num_train_steps=500000, log_interval=1000)
```
è¿è¡Œ`train.py`æ–‡ä»¶ï¼Œå°†ä¼šå¾—åˆ°å¦‚ä¸‹è¾“å‡ºï¼š

<div align=center>
<img src='./assets/images/rl_training_gpu.gif' style="filter: drop-shadow(0px 0px 7px #000);">
</div>

### ä½¿ç”¨HUAWEI NPU
ä¸ä¸Šè¿°æ¡ˆä¾‹ç±»ä¼¼, å¦‚æœéœ€è¦åœ¨ HUAWEI NPU ä¸Šè®­ç»ƒæ™ºèƒ½ä½“ï¼Œåªéœ€å°† `cuda` æ›¿æ¢ä¸º `npu`ï¼š
``` python
device = "cuda:0" -> device = "npu:0"
```

## ä¸‰æ­¥åˆ›å»ºæ‚¨çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
å€ŸåŠ©**RLLTE**ï¼Œå¼€å‘è€…åªéœ€ä¸‰æ­¥å°±å¯ä»¥å®ç°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚æ¥ä¸‹æ¥è¿™ä¸ªä¾‹å­å°†å±•ç¤ºå¦‚ä½•å®ç° Advantage Actor-Critic (A2C) ç®—æ³•ç”¨äºè§£å†³ Atari æ¸¸æˆï¼š 
- é¦–å…ˆï¼Œè°ƒç”¨ç®—æ³•åŸå‹ï¼š
``` py
from rllte.common.prototype import OnPolicyAgent
```
- å…¶æ¬¡ï¼Œå¯¼å…¥å¿…è¦çš„æ¨¡å—ï¼š
``` py
from rllte.xploit.encoder import MnihCnnEncoder
from rllte.xploit.policy import OnPolicySharedActorCritic
from rllte.xploit.storage import VanillaRolloutStorage
from rllte.xplore.distribution import Categorical
```
- è¿è¡Œé€‰å®šç­–ç•¥çš„ `.describe` å‡½æ•°ï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼š
``` py
OnPolicySharedActorCritic.describe()
# Output:
# ================================================================================
# Name       : OnPolicySharedActorCritic
# Structure  : self.encoder (shared by actor and critic), self.actor, self.critic
# Forward    : obs -> self.encoder -> self.actor -> actions
#            : obs -> self.encoder -> self.critic -> values
#            : actions -> log_probs
# Optimizers : self.optimizers['opt'] -> (self.encoder, self.actor, self.critic)
# ================================================================================
```
è¿™å°†ä¼šå±•ç¤ºå½“å‰ç­–ç•¥çš„æ•°æ®ç»“æ„ã€‚æœ€åï¼Œå°†ä¸Šè¿°æ¨¡å—æ•´åˆåˆ°ä¸€èµ·å¹¶ä¸”ç¼–å†™ `.update` å‡½æ•°:
``` py
from torch import nn
import torch as th

class A2C(OnPolicyAgent):
    def __init__(self, env, tag, seed, device, num_steps) -> None:
        super().__init__(env=env, tag=tag, seed=seed, device=device, num_steps=num_steps)
        # åˆ›å»ºæ¨¡å—
        encoder = MnihCnnEncoder(observation_space=env.observation_space, feature_dim=512)
        policy = OnPolicySharedActorCritic(observation_space=env.observation_space,
                                           action_space=env.action_space,
                                           feature_dim=512,
                                           opt_class=th.optim.Adam,
                                           opt_kwargs=dict(lr=2.5e-4, eps=1e-5),
                                           init_fn="xavier_uniform"
                                           )
        storage = VanillaRolloutStorage(observation_space=env.observation_space,
                                        action_space=env.action_space,
                                        device=device,
                                        storage_size=self.num_steps,
                                        num_envs=self.num_envs,
                                        batch_size=256
                                        )
        # è®¾å®šæ‰€æœ‰æ¨¡å—
        self.set(encoder=encoder, policy=policy, storage=storage, distribution=Categorical)
    
    def update(self):
        for _ in range(4):
            for batch in self.storage.sample():
                # è¯„ä¼°é‡‡æ ·çš„åŠ¨ä½œ
                new_values, new_log_probs, entropy = self.policy.evaluate_actions(obs=batch.observations, actions=batch.actions)
                # ç­–ç•¥æŸå¤±
                policy_loss = - (batch.adv_targ * new_log_probs).mean()
                # ä»·å€¼æŸå¤±
                value_loss = 0.5 * (new_values.flatten() - batch.returns).pow(2).mean()
                # æ›´æ–°
                self.policy.optimizers['opt'].zero_grad(set_to_none=True)
                (value_loss * 0.5 + policy_loss - entropy * 0.01).backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)
                self.policy.optimizers['opt'].step()
```
ç„¶åï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç è®­ç»ƒè¯¥æ™ºèƒ½ä½“ï¼š
``` py
from rllte.env import make_atari_env
if __name__ == "__main__":
    device = "cuda"
    env = make_atari_env("PongNoFrameskip-v4", num_envs=8, seed=0, device=device)
    agent = A2C(env=env, tag="a2c_atari", seed=0, device=device, num_steps=128)
    agent.train(num_train_steps=10000000)
```
ä¸Šè¿°ä¾‹å­è¡¨æ˜ï¼Œåˆ©ç”¨ **RLLTE** åªéœ€å°‘æ•°å‡ è¡Œä»£ç ä¾¿å¯ä»¥å¾—åˆ°ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ã€‚

## ç®—æ³•è§£è€¦ä¸æ¨¡å—æ›¿ä»£
**RLLTE** è®¸å¯å¼€å‘è€…å°†é¢„è®¾å¥½çš„æ¨¡å—æ›¿æ¢ï¼Œä»¥ä¾¿äºè¿›è¡Œç®—æ³•æ€§èƒ½æ¯”è¾ƒå’Œä¼˜åŒ–ã€‚å¼€å‘è€…å¯ä»¥å°†é¢„è®¾æ¨¡å—æ›¿æ¢æˆåˆ«çš„ç±»å‹çš„å†…ç½®æ¨¡å—æˆ–è€…è‡ªå®šä¹‰æ¨¡å—ã€‚å‡è®¾æˆ‘ä»¬æƒ³è¦å¯¹æ¯”ä¸åŒç¼–ç å™¨çš„æ•ˆæœï¼Œåªéœ€è¦è°ƒç”¨å…¶ä¸­ `.set` å‡½æ•°ï¼š
``` py
from rllte.xploit.encoder import EspeholtResidualEncoder
encoder = EspeholtResidualEncoder(...)
agent.set(encoder=encoder)
```
**RLLTE** æ¡†æ¶ååˆ†ç®€ä¾¿ï¼Œç»™äºˆå¼€å‘è€…ä»¬æœ€å¤§ç¨‹åº¦çš„è‡ªç”±ã€‚æ›´å¤šè¯¦ç»†è¯´æ˜è¯·å‚è€ƒ[æ•™ç¨‹](https://docs.rllte.dev/tutorials)ã€‚

# åŠŸèƒ½åˆ—è¡¨ (éƒ¨åˆ†)
## å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
|     ç±»å‹    |  ç®—æ³• | è¿ç»­ | ç¦»æ•£ | å¤šé‡äºŒå…ƒ | å¤šé‡ç¦»æ•£ | å¤šè¿›ç¨‹ | NPU |ğŸ’°|ğŸ”­|
|:-----------:|:------:|:---:|:----:|:----:|:----:|:------:|:---:|:------:|:---:|
| On-Policy   | [A2C](https://arxiv.org/abs/1602.01783)    | âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| On-Policy   | [PPO](https://arxiv.org/pdf/1707.06347)    | âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| On-Policy   | [DrAC](https://proceedings.neurips.cc/paper/2021/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âœ”ï¸   |
| On-Policy   | [DAAC](http://proceedings.mlr.press/v139/raileanu21a/raileanu21a.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âŒ   |
| On-Policy   | [DrDAAC](https://proceedings.neurips.cc/paper/2021/file/2b38c2df6a49b97f706ec9148ce48d86-Paper.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âœ”ï¸   |
| On-Policy   | [PPG](http://proceedings.mlr.press/v139/cobbe21a/cobbe21a.pdf)| âœ”ï¸   | âœ”ï¸    | âœ”ï¸    |  âŒ   | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âŒ   |
| Off-Policy  | [DQN](https://training.incf.org/sites/default/files/2023-05/Human-level%20control%20through%20deep%20reinforcement%20learning.pdf) | âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    | âŒ   |
| Off-Policy  | [DDPG](https://arxiv.org/pdf/1509.02971.pdf?source=post_page---------------------------)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| Off-Policy  | [SAC](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| Off-Policy  | [TD3](http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âœ”ï¸    | âœ”ï¸   |âœ”ï¸    |âŒ    |
| Off-Policy  | [DrQ-v2](https://arxiv.org/pdf/2107.09645.pdf?utm_source=morioh.com)| âœ”ï¸   | âŒ    | âŒ    | âŒ    | âŒ    | âœ”ï¸   |âœ”ï¸    |âœ”ï¸    |
| Distributed | [IMPALA](http://proceedings.mlr.press/v80/espeholt18a/espeholt18a.pdf) | âœ”ï¸   | âœ”ï¸    | âŒ    | âŒ    | âœ”ï¸    | âŒ   |âŒ    |âŒ    |

> - ğŸŒï¼šå¼€å‘ä¸­ï¼›
> - ğŸ’°ï¼šæ”¯æŒå†…åœ¨å¥–åŠ±å¡‘é€ ï¼›
> - ğŸ”­ï¼šæ”¯æŒè§‚æµ‹å¢å¼ºã€‚


## å†…åœ¨å¥–åŠ±æ¨¡å—
| **ç±»å‹** 	| **æ¨¡å—** 	|
|---	|---	|
| Count-based 	| [PseudoCounts](https://arxiv.org/pdf/2002.06038), [RND](https://arxiv.org/pdf/1810.12894.pdf) 	|
| Curiosity-driven 	| [ICM](http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf), [GIRM](http://proceedings.mlr.press/v119/yu20d/yu20d.pdf), [RIDE](https://arxiv.org/pdf/2002.12292) 	|
| Memory-based 	| [NGU](https://arxiv.org/pdf/2002.06038) 	|
| Information theory-based 	| [RE3](http://proceedings.mlr.press/v139/seo21a/seo21a.pdf), [RISE](https://ieeexplore.ieee.org/abstract/document/9802917/), [REVD](https://openreview.net/pdf?id=V2pw1VYMrDo) 	|

è¯¦ç»†æ¡ˆä¾‹è¯·å‚è€ƒ [Tutorials: Use Intrinsic Reward and Observation Augmentation](https://docs.rllte.dev/tutorials/data_augmentation)ã€‚

# RLLTE ç”Ÿæ€ç¯å¢ƒ
æ¢ç´¢**RLLTE**ç”Ÿæ€ä»¥åŠ é€Ÿæ‚¨çš„ç ”ç©¶ï¼š

- [Hub](https://docs.rllte.dev/benchmarks/)ï¼šæä¾›å¿«é€Ÿè®­ç»ƒçš„ API æ¥å£ä»¥åŠå¯é‡å¤ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ï¼›
- [Evaluation](https://docs.rllte.dev/api/tutorials/)ï¼šæä¾›å¯ä¿¡èµ–çš„æ¨¡å‹è¯„ä¼°æ ‡å‡†ï¼›
- [Env](https://docs.rllte.dev/api/tutorials/)ï¼šæä¾›å°è£…å®Œå–„çš„ç¯å¢ƒï¼›
- [Deployment](https://docs.rllte.dev/api/tutorials/)ï¼šæä¾›ä¾¿æ·çš„ç®—æ³•éƒ¨ç½²æ¥å£ï¼›
- [Pre-training](https://docs.rllte.dev/api/tutorials/)ï¼šæä¾›å¤šç§å¼ºåŒ–å­¦ä¹ é¢„è®­ç»ƒçš„æ–¹å¼ï¼›
- [Copilot](https://docs.rllte.dev/copilot)ï¼šæä¾›å¤§è¯­è¨€æ¨¡å‹ copilotã€‚

# API æ–‡æ¡£
è¯·å‚é˜…æˆ‘ä»¬ä¾¿æ·çš„ API æ–‡æ¡£ï¼š[https://docs.rllte.dev/](https://docs.rllte.dev/)
<div align=center>
<img src='./assets/images/docs.gif' style="width: 100%">
</div>

# å¦‚ä½•è´¡çŒ®
æ¬¢è¿å‚ä¸è´¡çŒ®æˆ‘ä»¬çš„é¡¹ç›®ï¼åœ¨æ‚¨å‡†å¤‡ç¼–ç¨‹ä¹‹å‰ï¼Œè¯·å…ˆå‚é˜…[CONTRIBUTING.md](https://github.com/RLE-Foundation/rllte/blob/main/CONTRIBUTING.md)ã€‚

# å¼•ç”¨é¡¹ç›®
å¦‚æœæ‚¨æƒ³åœ¨ç ”ç©¶ä¸­å¼•ç”¨ **RLLTE**ï¼Œè¯·å‚è€ƒå¦‚ä¸‹æ ¼å¼ï¼š
``` tex
@software{rllte,
  author = {Mingqi Yuan, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, and Wenjun Zeng},
  title = {RLLTE: Long-Term Evolution Project of Reinforcement Learning},
  url = {https://github.com/RLE-Foundation/rllte},
  year = {2023},
}
```

# è‡´è°¢
è¯¥é¡¹ç›®ç”± [é¦™æ¸¯ç†å·¥å¤§å­¦](http://www.polyu.edu.hk/)ï¼Œ[ä¸œæ–¹ç†å·¥é«˜ç­‰ç ”ç©¶é™¢](http://www.eias.ac.cn/)ï¼Œä»¥åŠ [FLW-Foundation](FLW-Foundation)èµåŠ©ã€‚ [ä¸œæ–¹ç†å·¥é«˜æ€§èƒ½è®¡ç®—ä¸­å¿ƒ](https://hpc.eias.ac.cn/) æä¾›äº† GPU è®¡ç®—å¹³å°, [åä¸ºå¼‚è…¾](https://www.hiascend.com/) æä¾›äº† NPU è®¡ç®—å¹³å°ã€‚è¯¥é¡¹ç›®çš„éƒ¨åˆ†ä»£ç å‚è€ƒäº†å…¶ä»–ä¼˜ç§€çš„å¼€æºé¡¹ç›®ï¼Œè¯·å‚è§ [ACKNOWLEDGMENT.md](https://github.com/RLE-Foundation/rllte/blob/main/ACKNOWLEDGMENT.md)ã€‚
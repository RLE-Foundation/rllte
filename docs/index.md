---
hide:
  - toc
---

# **RLLTE: Long-Term Evolution Project of Reinforcement Learning**



---

Inspired by the long-term evolution (LTE) standard project in telecommunications, aiming to provide development components and standards for advancing RL research and applications. Beyond delivering top-notch algorithm implementations, **RLLTE** also serves as a **toolkit** for developing algorithms.

<div align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/PMF6fa72bmE?si=oDLvQqxVrMP31Iqk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<br>
An introduction to RLLTE.
</div>

## **Why RLLTE?**
- ğŸ§¬ Long-term evolution for providing latest algorithms and tricks;
- ğŸï¸ Complete ecosystem for task design, model training, evaluation, and deployment (TensorRT, CANN, ...);
- ğŸ§± Module-oriented design for complete decoupling of RL algorithms;
- ğŸš€ Optimized workflow for full hardware acceleration;
- âš™ï¸ Support custom environments and modules;
- ğŸ–¥ï¸ Support multiple computing devices like GPU and NPU;
- ğŸ’¾ Large number of reusable benchmarks (See [rllte-hub](https://hub.rllte.dev));
- ğŸ‘¨â€âœˆï¸ Large language model-empowered copilot.

## **A `PyTorch` for RL**
RLLTE decouples RL algorithms into minimum primitives and provide standard modules for development. 

See [Fast Algorithm Development]() for detailed examples.
<div align=left>
<img src='./assets/images/structure.svg' style="width: 80%">
</div>


## **Project Evolution**
**RLLTE** selects RL algorithms based on the following tenet:

- Generality is the most important;
- Improvements in sample efficiency or generalization ability;
- Excellent performance on recognized benchmarks;
- Promising tools for RL.

## **Cite Us**
If you use **RLLTE** in your research, please cite this project like this:
```bibtex
@article{yuan2023rllte,
  title={RLLTE: Long-Term Evolution Project of Reinforcement Learning}, 
  author={Mingqi Yuan and Zequn Zhang and Yang Xu and Shihao Luo and Bo Li and Xin Jin and Wenjun Zeng},
  year={2023},
  journal={arXiv preprint arXiv:2309.16382}
}
```
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea927e79-a5a9-4dc8-91b1-f917883fb6af",
   "metadata": {},
   "source": [
    "Developers only need three steps to implement an RL algorithm with **RLLTE**:\n",
    "\n",
    "1. Selection an algorithm prototype;\n",
    "2. Select desired modules;\n",
    "3. Write a update function.\n",
    "\n",
    "The following example illustrates how to write an Advantage Actor-Critic (A2C) agent to solve Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edefe54f-8a57-4da5-8b5a-f062aa7f9435",
   "metadata": {},
   "source": [
    "# 1. Set prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526e1bc-0704-41e7-9f3c-9ed7f71f7b95",
   "metadata": {},
   "source": [
    "Firstly, we select `OnPolicyAgent` as the prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c422ba-ce0f-49a9-9448-daf8cf57d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rllte-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88f76fb-87c1-4546-8367-8b8b51f39ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.4.0 (SDL 2.26.4, Python 3.8.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from rllte.common.prototype import OnPolicyAgent\n",
    "\n",
    "class A2C(OnPolicyAgent):\n",
    "    def __init__(self, env, tag, device, num_steps):\n",
    "        # here we only use four arguments\n",
    "        super().__init__(env=env, tag=tag, device=device, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ab2fd-5985-415e-8648-fbbac64f0a95",
   "metadata": {},
   "source": [
    "# 2. Set necessary modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807f605-0559-4862-b9c6-9f53b90b6d3f",
   "metadata": {},
   "source": [
    "Now we need an `encoder` to process observations, a learnable `policy` to generate actions, and a `storage` to store and sample experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b5651b-bfca-4d7a-a284-30beab36465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllte.xploit.encoder import MnihCnnEncoder\n",
    "from rllte.xploit.policy import OnPolicySharedActorCritic\n",
    "from rllte.xploit.storage import VanillaRolloutStorage\n",
    "from rllte.xplore.distribution import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f04ca-6f18-4532-a367-3991e97eb7d0",
   "metadata": {},
   "source": [
    "# 3. Set update function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ee02d-2958-4bb9-848e-3a86cb2ce306",
   "metadata": {},
   "source": [
    "Run the `.describe` function of the selected policy and you will see the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5309d9a9-30ad-4bb5-89d8-02c9926c4808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "Name       : OnPolicySharedActorCritic\n",
      "Structure  : self.encoder (shared by actor and critic), self.actor, self.critic\n",
      "Forward    : obs -> self.encoder -> self.actor -> actions\n",
      "           : obs -> self.encoder -> self.critic -> values\n",
      "           : actions -> log_probs\n",
      "Optimizers : self.optimizers['opt'] -> (self.encoder, self.actor, self.critic)\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OnPolicySharedActorCritic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e733456-0798-47b1-b6ac-ca28f60adbb0",
   "metadata": {},
   "source": [
    "This will illustrate the structure of the policy and indicate the optimizable parts. Finally, merge these modules and write a `.update` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84a61639-b8d5-489a-9d33-dee8a682efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch as th\n",
    "\n",
    "class A2C(OnPolicyAgent):\n",
    "    def __init__(self, env, tag, seed, device, num_steps) -> None:\n",
    "        super().__init__(env=env, tag=tag, seed=seed, device=device, num_steps=num_steps)\n",
    "        # create modules\n",
    "        encoder = MnihCnnEncoder(observation_space=env.observation_space, feature_dim=512)\n",
    "        policy = OnPolicySharedActorCritic(observation_space=env.observation_space,\n",
    "                                           action_space=env.action_space,\n",
    "                                           feature_dim=512,\n",
    "                                           opt_class=th.optim.Adam,\n",
    "                                           opt_kwargs=dict(lr=2.5e-4, eps=1e-5),\n",
    "                                           init_fn=\"xavier_uniform\"\n",
    "                                           )\n",
    "        storage = VanillaRolloutStorage(observation_space=env.observation_space,\n",
    "                                        action_space=env.action_space,\n",
    "                                        device=device,\n",
    "                                        storage_size=self.num_steps,\n",
    "                                        num_envs=self.num_envs,\n",
    "                                        batch_size=256\n",
    "                                        )\n",
    "        # set all the modules\n",
    "        self.set(encoder=encoder, policy=policy, storage=storage, distribution=Categorical)\n",
    "    \n",
    "    def update(self):\n",
    "        for _ in range(4):\n",
    "            for batch in self.storage.sample():\n",
    "                # evaluate the sampled actions\n",
    "                new_values, new_log_probs, entropy = self.policy.evaluate_actions(obs=batch.observations, actions=batch.actions)\n",
    "                # policy loss part\n",
    "                policy_loss = - (batch.adv_targ * new_log_probs).mean()\n",
    "                # value loss part\n",
    "                value_loss = 0.5 * (new_values.flatten() - batch.returns).pow(2).mean()\n",
    "                # update\n",
    "                self.policy.optimizers['opt'].zero_grad(set_to_none=True)\n",
    "                (value_loss * 0.5 + policy_loss - entropy * 0.01).backward()\n",
    "                nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.policy.optimizers['opt'].step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e777a-9fbb-4887-9b69-f51e2b8ab1e9",
   "metadata": {},
   "source": [
    "# 4. Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67d8c3-65c0-4d59-ac70-60346af610b0",
   "metadata": {},
   "source": [
    "Now we can start training by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca18442-a5aa-4186-857f-d0e69f0cbacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Invoking RLLTE Engine...\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - ================================================================================\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Tag               : a2c_atari\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Device            : NVIDIA GeForce RTX 3090\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Agent             : A2C\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Encoder           : MnihCnnEncoder\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Policy            : OnPolicySharedActorCritic\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Storage           : VanillaRolloutStorage\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Distribution      : Categorical\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Augmentation      : False\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - Intrinsic Reward  : False\n",
      "[08/28/2023 05:43:19 PM] - [\u001b[1m\u001b[33mDEBUG\u001b[0m] - ================================================================================\n",
      "[08/28/2023 05:43:22 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 1024        | E: 8           | L: 128         | R: 50.000      | FPS: 241.297   | T: 0:00:04    \n",
      "[08/28/2023 05:43:23 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 2048        | E: 16          | L: 169         | R: 75.000      | FPS: 432.725   | T: 0:00:04    \n",
      "[08/28/2023 05:43:23 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 3072        | E: 24          | L: 227         | R: 78.889      | FPS: 587.923   | T: 0:00:05    \n",
      "[08/28/2023 05:43:24 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 4096        | E: 32          | L: 264         | R: 92.000      | FPS: 721.760   | T: 0:00:05    \n",
      "[08/28/2023 05:43:24 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 5120        | E: 40          | L: 214         | R: 82.000      | FPS: 832.101   | T: 0:00:06    \n",
      "[08/28/2023 05:43:25 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 6144        | E: 48          | L: 186         | R: 77.000      | FPS: 920.717   | T: 0:00:06    \n",
      "[08/28/2023 05:43:25 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 7168        | E: 56          | L: 228         | R: 98.000      | FPS: 997.503   | T: 0:00:07    \n",
      "[08/28/2023 05:43:26 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 8192        | E: 64          | L: 251         | R: 124.000     | FPS: 1071.546  | T: 0:00:07    \n",
      "[08/28/2023 05:43:26 PM] - [\u001b[1m\u001b[31mTRAIN\u001b[0m] - S: 9216        | E: 72          | L: 240         | R: 124.000     | FPS: 1126.947  | T: 0:00:08    \n",
      "[08/28/2023 05:43:26 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Training Accomplished!\n",
      "[08/28/2023 05:43:26 PM] - [\u001b[1m\u001b[34mINFO.\u001b[0m] - Model saved at: /export/yuanmingqi/code/rllte/examples/logs/a2c_atari/2023-08-28-05-43-18/model\n"
     ]
    }
   ],
   "source": [
    "from rllte.env import make_atari_env\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\"\n",
    "    env = make_atari_env(\"AlienNoFrameskip-v4\", num_envs=8, seed=0, device=device)\n",
    "    agent = A2C(env=env, tag=\"a2c_atari\", seed=0, device=device, num_steps=128)\n",
    "    agent.train(num_train_steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa388720-b9f4-4308-bad5-d0d083163623",
   "metadata": {},
   "source": [
    "As shown in this example, only a few dozen lines of code are needed to create RL agents with **RLLTE**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
